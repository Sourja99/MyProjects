{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourja99/MyProjects/blob/main/sequence_labelling_codeonly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRmWaic8hU0u",
        "outputId": "35619d74-afef-4444-9612-60bfc756975a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read the data in\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ee7cf674d01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#1) Read data in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read the data in\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0msentences_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_conll2003\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONLL2003/train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0msentences_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_conll2003\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONLL2003/valid.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ee7cf674d01e>\u001b[0m in \u001b[0;36mread_conll2003\u001b[0;34m(f_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"Yield complete sentences\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcurrent_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#This will be a list of (word,label), which we accumulate for each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#drop whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CONLL2003/train.txt'"
          ]
        }
      ],
      "source": [
        "from collections import namedtuple\n",
        "import sklearn.svm\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "\n",
        "#Same as tuple but the fields are named for convenience\n",
        "#this says we have four fields\n",
        "OneWord=namedtuple(\"OneWord\",[\"word\",\"pos_label\",\"chunk_label\",\"entity_label\"])\n",
        "\n",
        "def read_conll2003(f_name):\n",
        "    \"\"\"Yield complete sentences\"\"\"\n",
        "    current_sentence=[] #This will be a list of (word,label), which we accumulate for each sentence\n",
        "    with open(f_name) as f:\n",
        "        for line in f:\n",
        "            line=line.strip() #drop whitespace\n",
        "            if line.startswith(\"-DOCSTART-\"): #let's not worry about these for the time being\n",
        "                continue\n",
        "            if not line: #sentence break\n",
        "                if current_sentence: #if we gathered a sentence, we should yield it, because a new starts\n",
        "                    yield current_sentence #much like return, but continues past this line once the element has been consumed\n",
        "                    current_sentence=[] #...and start a new one\n",
        "                continue\n",
        "            #if we made it here, we are on a normal line\n",
        "            columns=line.split() #an actual word line\n",
        "            assert len(columns)==4 #we should have four columns, looking at the data\n",
        "            current_sentence.append(OneWord(*columns))\n",
        "        else: #for ... else -> the else part is executed once, when \"for\" runs out of elements\n",
        "            if current_sentence: #yield also the last one!\n",
        "                yield current_sentence\n",
        "                \n",
        "def generate_sentence_features(sent):\n",
        "    #Given a sentence as a list of (word, label) pairs\n",
        "    #generate the features for every word\n",
        "    #The result should be a list of same length as the sentence\n",
        "    #Each item is a dictionary of {\"feature name\"->feature value} mappings, holding all features of the word at that position\n",
        "    \n",
        "    sent_features=[] #this will be the result\n",
        "    for word_idx, one_word in enumerate(sent):\n",
        "        #We do nothing with label\n",
        "        #it just happens to be around\n",
        "        word_features={}\n",
        "        word_features[\"word_\"+one_word.word]=1 #the word itself is a feature\n",
        "        if word_idx!=0:\n",
        "            word_features[\"left_word_\"+sent[word_idx-1].word]=1\n",
        "        if word_idx!=len(sent)-1:\n",
        "            word_features[\"right_word_\"+sent[word_idx+1].word]=1\n",
        "        sent_features.append(word_features)\n",
        "    return sent_features\n",
        "\n",
        "def prep_data(sentences):\n",
        "    all_labels=[] #here we gather labels for all words in all sentences\n",
        "    all_features=[] #here we gather features for all words in all sentences\n",
        "    for sentence in sentences:\n",
        "        sent_features=generate_sentence_features(sentence)\n",
        "        assert len(sent_features)==len(sentence)\n",
        "        #Now we can get, for every position its label and its features\n",
        "        for one_word,features in zip(sentence,sent_features):\n",
        "            all_labels.append(one_word.pos_label) #label\n",
        "            all_features.append(features)         #and features to go with it\n",
        "    return all_labels, all_features\n",
        "\n",
        "\n",
        "#1) Read data in\n",
        "print(\"Read the data in\")\n",
        "sentences_train=list(read_conll2003(\"CONLL2003/train.txt\"))\n",
        "sentences_dev=list(read_conll2003(\"CONLL2003/valid.txt\"))\n",
        "\n",
        "#2) Generate the features and labels\n",
        "print(\"Generate features\")\n",
        "train_labels,train_features=prep_data(sentences_train)\n",
        "dev_labels,dev_features=prep_data(sentences_dev)\n",
        "\n",
        "#3) Prepare the feature vectors for the classifier\n",
        "print(\"Generate feature vectors\")\n",
        "vectorizer=DictVectorizer()\n",
        "vectorizer.fit(train_features)\n",
        "feature_vectors_train=vectorizer.transform(train_features)\n",
        "feature_vectors_dev=vectorizer.transform(dev_features)\n",
        "\n",
        "#4) Train the classifier\n",
        "print(\"Train the classifier (takes a while for C=0.5 and higher)\")\n",
        "classifier=sklearn.svm.LinearSVC(C=0.5,verbose=1)\n",
        "classifier.fit(feature_vectors_train, train_labels)\n",
        "\n",
        "#5) Evaluate\n",
        "print(\"Evaluate\")\n",
        "classifier.score(feature_vectors_dev,dev_labels)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "sequence_labelling_codeonly.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}